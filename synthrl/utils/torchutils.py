import torch
import torch.nn as nn

class Attention(nn.Module):
  def __init__(self, dimensions=0):
    super(Attention, self).__init__()    

  def forward(self):
    raise NotImplementedError
